{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# Lab | Chains in LangChain\n",
    "\n",
    "## Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541eb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b84e441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7a09c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                                             Review\n",
       "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
       "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
       "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
       "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
       "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940ce7c",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e92dff22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943237a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None by your own value and justify\n",
    "llm = ChatOpenAI(temperature = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdcdb42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input_parameter'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_parameter'], input_types={}, partial_variables={}, template='Write me an outline on {input_parameter}?'), additional_kwargs={})]\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template( # Write a query that would take a variable to describe any product\n",
    "    \"Write me an outline on {input_parameter}?\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "print(type(prompt))\n",
    "\n",
    "'''\n",
    "input_variables = ['input_parameter'] \n",
    "input_types = {} \n",
    "partial_variables = {} \n",
    "messages = [ HumanMessagePromptTemplate( prompt = PromptTemplate(input_variables = ['input_parameter'],\n",
    "                                                                 input_types = {},\n",
    "                                                                 partial_variables = {}, \n",
    "                                                                 template = 'Write me an outline on {input_parameter}?'), \n",
    "                                                                 additional_kwargs = {} ) ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7abc20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose=False prompt=ChatPromptTemplate(input_variables=['input_parameter'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_parameter'], input_types={}, partial_variables={}, template='Write me an outline on {input_parameter}?'), additional_kwargs={})]) llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001DFAB59FA10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DFAB5B3410>, root_client=<openai.OpenAI object at 0x000001DFAB582810>, root_async_client=<openai.AsyncOpenAI object at 0x000001DFAB59F4D0>, model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={}\n",
      "<class 'langchain.chains.llm.LLMChain'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lexo\\AppData\\Local\\Temp\\ipykernel_43788\\90996734.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm = llm, prompt = prompt)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain = LLMChain(llm = llm, prompt = prompt)\n",
    "\n",
    "print(chain)\n",
    "print(type(chain))\n",
    "\n",
    "'''\n",
    "verbose = False \n",
    "prompt = ChatPromptTemplate(input_variables = ['input_parameter'], \n",
    "                            input_types = {}, \n",
    "                            partial_variables = {}, \n",
    "                            messages = [HumanMessagePromptTemplate( prompt = PromptTemplate(input_variables = ['input_parameter'], \n",
    "                                                                                            input_types = {}, \n",
    "                                                                                            partial_variables = {}, \n",
    "                                                                                            template = 'Write me an outline on {input_parameter}?'), \n",
    "                                                                                            additional_kwargs = {} )])\n",
    "\n",
    "llm = ChatOpenAI(client = <openai.resources.chat.completions.Completions object at 0x000001DFAB59FA10>, \n",
    "                 async_client = <openai.resources.chat.completions.AsyncCompletions object at 0x000001DFAB5B3410>, \n",
    "                 root_client = <openai.OpenAI object at 0x000001DFAB582810>, \n",
    "                 root_async_client = <openai.AsyncOpenAI object at 0x000001DFAB59F4D0>, \n",
    "                 model_kwargs = {}, \n",
    "                 openai_api_key = SecretStr('**********'))\n",
    "\n",
    "output_parser = StrOutputParser() \n",
    "llm_kwargs = {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad44d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lexo\\AppData\\Local\\Temp\\ipykernel_43788\\1985126359.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(product)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I. Introduction\\n  A. Definition of a mask\\n  B. Importance of masks in various cultures and societies\\n  C. Purpose of wearing masks\\n\\nII. History of masks\\n  A. Origins of masks\\n  B. Evolution of masks in different cultures\\n  C. Significance of masks in ancient civilizations\\n\\nIII. Types of masks\\n  A. Cultural masks\\n    1. African masks\\n    2. Japanese Noh masks\\n    3. Native American masks\\n  B. Protective masks\\n    1. Surgical masks\\n    2. Respirator masks\\n    3. Gas masks\\n\\nIV. Materials used in making masks\\n  A. Traditional materials\\n    1. Wood\\n    2. Clay\\n    3. Leather\\n  B. Modern materials\\n    1. Plastic\\n    2. Fabric\\n    3. Metal\\n\\nV. Symbolism of masks\\n  A. Expression of identity\\n  B. Role in rituals and ceremonies\\n  C. Representation of emotions and characters\\n\\nVI. Use of masks in popular culture\\n  A. Theatre and performing arts\\n  B. Halloween and costume parties\\n  C. Fashion and streetwear\\n\\nVII. Conclusion\\n  A. Recap of the significance and diversity of masks\\n  B. Final thoughts on the enduring appeal of masks in human societies.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"mask\" # Select a product type to be describe\n",
    "chain.run(product)\n",
    "\n",
    "'''\n",
    "I. Introduction\\n\n",
    "A. Definition of a mask\\n \n",
    "B. Importance of masks in various cultures and societies\\n\n",
    "C. Purpose of wearing masks\\n\\n\n",
    "\n",
    "II. History of masks\\n\n",
    "A. Origins of masks\\n\n",
    "B. Evolution of masks in different cultures\\n\n",
    "C. Significance of masks in ancient civilizations\\n\\n\n",
    "\n",
    "III. Types of masks\\n\n",
    "A. Cultural masks\\n\n",
    "1. African masks\\n\n",
    "2. Japanese Noh masks\\n\n",
    "3. Native American masks\\n\n",
    "B. Protective masks\\n \n",
    "1. Surgical masks\\n\n",
    "2. Respirator masks\\n\n",
    "3. Gas masks\\n\\n\n",
    "\n",
    "IV. Materials used in making masks\\n\n",
    "A. Traditional materials\\n\n",
    "1. Wood\\n\n",
    "2. Clay\\n\n",
    "3. Leather\\n\n",
    "B. Modern materials\\n\n",
    "1. Plastic\\n\n",
    "2. Fabric\\n\n",
    "3. Metal\\n\\n\n",
    "\n",
    "V. Symbolism of masks\\n\n",
    "A. Expression of identity\\n\n",
    "B. Role in rituals and ceremonies\\n\n",
    "C. Representation of emotions and characters\\n\\n\n",
    "VI. Use of masks in popular culture\\n\n",
    "A. Theatre and performing arts\\n\n",
    "B. Halloween and costume parties\\n\n",
    "C. Fashion and streetwear\\n\\n\n",
    "\n",
    "VII. Conclusion\\n\n",
    "A. Recap of the significance and diversity of masks\\n\n",
    "B. Final thoughts on the enduring appeal of masks in human societies.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03469",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "febee243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f31aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.9)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write me an outline on {input_parameter}?\" # Repeat the initial query or create a new query that would feed into the second prompt\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm = llm, prompt = first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f5d5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write me 5 famous {input_parameter}?\" # Write the second prompt query that takes an input variable whose input will come from the previous prompt\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm = llm, prompt = second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c1eb2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose=True chains=[LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['input_parameter'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_parameter'], input_types={}, partial_variables={}, template='Write me an outline on {input_parameter}?'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001DFAB5F9CD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DFAB5ECC80>, root_client=<openai.OpenAI object at 0x000001DFAB5F8140>, root_async_client=<openai.AsyncOpenAI object at 0x000001DFAB5B2420>, temperature=0.9, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['input_parameter'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_parameter'], input_types={}, partial_variables={}, template='Write me 5 famous {input_parameter}?'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001DFAB5F9CD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DFAB5ECC80>, root_client=<openai.OpenAI object at 0x000001DFAB5F8140>, root_async_client=<openai.AsyncOpenAI object at 0x000001DFAB5B2420>, temperature=0.9, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={})]\n",
      "<class 'langchain.chains.sequential.SimpleSequentialChain'>\n"
     ]
    }
   ],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains = [chain_one, chain_two],\n",
    "                                             verbose = True\n",
    "                                            )\n",
    "\n",
    "print(overall_simple_chain)\n",
    "print(type(overall_simple_chain))\n",
    "\n",
    "'''\n",
    "verbose = True\n",
    "chains = [LLMChain(verbose = False, \n",
    "                   prompt = ChatPromptTemplate(input_variables = ['input_parameter'], \n",
    "                                               input_types = {}, \n",
    "                                               partial_variables = {}, \n",
    "                                               messages = [HumanMessagePromptTemplate(prompt = PromptTemplate(input_variables = ['input_parameter'], \n",
    "                                                                                                              input_types = {}, \n",
    "                                                                                                              partial_variables = {}, \n",
    "                                                                                                              template = 'Write me an outline on {input_parameter}?'), \n",
    "                                                                                      additional_kwargs = {} )]), \n",
    "                    llm = ChatOpenAI(client = <openai.resources.chat.completions.Completions object at 0x000001DFAB5F9CD0>,\n",
    "                    async_client = <openai.resources.chat.completions.AsyncCompletions object at 0x000001DFAB5ECC80>,\n",
    "                    root_client = <openai.OpenAI object at 0x000001DFAB5F8140>,\n",
    "                                root_async_client = <openai.AsyncOpenAI object at 0x000001DFAB5B2420>,\n",
    "                                temperature = 0.9,\n",
    "                                model_kwargs = {},\n",
    "                                openai_api_key = SecretStr('**********')),\n",
    "                    output_parser = StrOutputParser(), \n",
    "                    llm_kwargs = {}), \n",
    "                    \n",
    "        LLMChain(verbose = False, \n",
    "                prompt = ChatPromptTemplate(input_variables = ['input_parameter'], \n",
    "                                            input_types = {}, \n",
    "                                            partial_variables = {}, \n",
    "                                            messages = [HumanMessagePromptTemplate(prompt = PromptTemplate(input_variables = ['input_parameter'], \n",
    "                                                                                                           input_types = {}, \n",
    "                                                                                                           partial_variables = {}, \n",
    "                                                                                                           template = 'Write me 5 famous {input_parameter}?'), \n",
    "                                                                                    additional_kwargs = {} )]), \n",
    "                llm = ChatOpenAI(client = <openai.resources.chat.completions.Completions object at 0x000001DFAB5F9CD0>, \n",
    "                                 async_client = <openai.resources.chat.completions.AsyncCompletions object at 0x000001DFAB5ECC80>, \n",
    "                                 root_client = <openai.OpenAI object at 0x000001DFAB5F8140>, \n",
    "                                 root_async_client = <openai.AsyncOpenAI object at 0x000001DFAB5B2420>, \n",
    "                                 temperature = 0.9, \n",
    "                                 model_kwargs = {}, \n",
    "                                 openai_api_key = SecretStr('**********')), \n",
    "                output_parser = StrOutputParser(), \n",
    "                llm_kwargs = {} )]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78458efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mI. Introduction\n",
      "A. Definition of mask\n",
      "B. Importance of masks\n",
      "C. Purpose of masks\n",
      "\n",
      "II. History of masks\n",
      "A. Origins of masks\n",
      "B. Evolution of masks\n",
      "C. Cultural significance of masks\n",
      "\n",
      "III. Types of masks\n",
      "A. Protective masks\n",
      "1. Surgical masks\n",
      "2. N95 masks\n",
      "B. Costume masks\n",
      "1. Halloween masks\n",
      "2. Masquerade masks\n",
      "C. Ritual masks\n",
      "1. Indigenous masks\n",
      "2. Religious masks\n",
      "\n",
      "IV. Materials used in making masks\n",
      "A. Fabric masks\n",
      "B. Paper masks\n",
      "C. Clay masks\n",
      "D. Plastic masks\n",
      "E. Metal masks\n",
      "\n",
      "V. How to wear and care for masks\n",
      "A. Proper way to wear a mask\n",
      "B. How to properly clean and maintain masks\n",
      "C. How to dispose of masks\n",
      "\n",
      "VI. Cultural significance of masks\n",
      "A. Masks in different cultures\n",
      "B. Symbolism of masks\n",
      "C. Traditional uses of masks in ceremonies and events\n",
      "\n",
      "VII. Conclusion\n",
      "A. Recap of the importance and purpose of masks\n",
      "B. Final thoughts on the significance of masks in society\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "C. How wearing masks can help protect public health and promote safety\n",
      "D. The potential future trends of mask-wearing in society\n",
      "E. Encouragement for the continued use of masks as a preventative measure against illnesses\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nC. How wearing masks can help protect public health and promote safety\\nD. The potential future trends of mask-wearing in society\\nE. Encouragement for the continued use of masks as a preventative measure against illnesses'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd59bda-9d02-44e7-b3d6-2bec61b99d8f",
   "metadata": {},
   "source": [
    "**Repeat the above twice for different products**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ce18c",
   "metadata": {},
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c129ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "016187ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_variables=[\"title\", \"audience\", \"tone\"],\\n    template=\"\"\"This program will generate an introductory paragraph to a blog post given a blog title, audience, and tone of voice\\n\\n    Blog Title: {title}\\n    Audience: {audience}\\n    Tone of Voice: {tone}\"\"\"\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature = 0.9)\n",
    "\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "  \"Give me a translation of the following review {review} in spanish?\" # This prompt should translate a review\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm = llm, prompt = first_prompt, \n",
    "                     output_key = \"spanish_review\" # Give a name to your output\n",
    "                    )\n",
    "\n",
    "'''\n",
    "input_variables=[\"title\", \"audience\", \"tone\"],\n",
    "    template=\"\"\"This program will generate an introductory paragraph to a blog post given a blog title, audience, and tone of voice\n",
    "\n",
    "    Blog Title: {title}\n",
    "    Audience: {audience}\n",
    "    Tone of Voice: {tone}\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fb0730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Give me a summarization of the following review {spanish_review}?\" # Write a promplt to summarize a review\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm = llm, prompt = second_prompt, \n",
    "                     output_key = \"summary_review\" # give a name to this output\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6accf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english or other language\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Give me a translation of the following review {summary_review} in italian?\"\n",
    ")\n",
    "# chain 3: input = Review and output= language\n",
    "chain_three = LLMChain(llm = llm, prompt = third_prompt,\n",
    "                       output_key = \"italian_review\"\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7a46121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message that take as inputs the two previous prompts' variables\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Compare {review} and {summary_review} and tell me the differences.\"\n",
    ")\n",
    "chain_four = LLMChain(llm = llm, prompt = fourth_prompt,\n",
    "                      output_key = \"difference_review\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89603117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input = Review \n",
    "# and output = English_Review, summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains = [chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables = [\"review\"],\n",
    "    output_variables = [\"spanish_review\", \"summary_review\", \"difference_review\"],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51b04f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lexo\\AppData\\Local\\Temp\\ipykernel_43788\\1992003631.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  overall_chain(review)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'spanish_review': 'Encuentro el sabor mediocre. La espuma no dura, es extraño. Compro los mismos en el comercio y el sabor es mucho mejor... ¿Lote viejo o falsificación!?',\n",
       " 'summary_review': 'El autor encuentra que el sabor de este producto es mediocre y que la espuma no dura, lo cual le resulta extraño. Afirma que ha comprado el mismo producto en comercios y el sabor es mucho mejor, lo que lo lleva a preguntarse si se trata de un lote viejo o una falsificación.',\n",
       " 'difference_review': 'The first statement is in French and the second statement is in Spanish, but they convey similar sentiments. Both express dissatisfaction with the taste and quality of a product, specifically mentioning the lack of foam retention. The French statement also suggests the possibility of the product being old or counterfeit, while the Spanish statement directly questions whether it is an old batch or counterfeit.\\n\\nOverall, the main difference lies in the language used to convey the same dissatisfaction with the product. The French statement is more straightforward in mentioning the possibility of a fake or old product, while the Spanish statement poses it as a question. Additionally, the French statement uses more informal language with expressions like \"c\\'est bizarre\" (it\\'s strange), while the Spanish statement is more neutral in tone.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187cf07-458a-4226-bec7-3dec7ee47af2",
   "metadata": {},
   "source": [
    "**Repeat the above twice for different products or reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041ea4c",
   "metadata": {},
   "source": [
    "## Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ade83f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f590e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31b06fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3f50bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8eefec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics: Good for answering questions about physics\n",
      "math: Good for answering math questions\n",
      "History: Good for answering history questions\n",
      "computer science: Good for answering computer science questions\n"
     ]
    }
   ],
   "source": [
    "destination_chains = {}\n",
    "\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template = prompt_template)\n",
    "    chain = LLMChain(llm = llm, prompt = prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f98018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm = llm, prompt = default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11b2e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1387109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations = destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template = router_template,\n",
    "    input_variables = [\"input\"],\n",
    "    output_parser = RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2fb7d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lexo\\AppData\\Local\\Temp\\ipykernel_43788\\1993956254.py:1: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
      "  chain = MultiPromptChain(router_chain = router_chain,\n"
     ]
    }
   ],
   "source": [
    "chain = MultiPromptChain(\n",
    "    router_chain = router_chain,\n",
    "    destination_chains = destination_chains, \n",
    "    default_chain = default_chain,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d86b2131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quantum mechanics is a fundamental theory in physics that describes the behavior of particles at the smallest scales, such as atoms and subatomic particles. It is a branch of physics that deals with the behavior of matter and energy on the atomic and subatomic levels. In quantum mechanics, particles can exist in multiple states at the same time, known as superposition, and can also be entangled with each other, meaning their states are dependent on each other even when separated by large distances. Quantum mechanics has led to the development of technologies such as quantum computing and quantum cryptography, which have the potential to revolutionize various fields including information processing and communication.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is a quantum mechanics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b717379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29e5be01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every cell in our body contains DNA because DNA carries the genetic information that determines the characteristics and functions of each cell. DNA contains the instructions for building and maintaining the structures and processes necessary for life. It serves as a blueprint for the development, growth, and functioning of all living organisms. Additionally, DNA is essential for cell division, replication, and repair, ensuring the continuity and stability of genetic information across generations. Therefore, the presence of DNA in every cell is crucial for the proper functioning and survival of an organism.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0c60b-7ae0-453e-9467-142d8dafee6e",
   "metadata": {},
   "source": [
    "**Repeat the above at least once for different inputs and chains executions - Be creative!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
