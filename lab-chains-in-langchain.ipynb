{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# Lab | Chains in LangChain\n",
    "\n",
    "## Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefeacff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541eb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84e441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a09c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\r\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Product  \\\n",
       "0       Queen Size Sheet Set   \n",
       "1     Waterproof Phone Pouch   \n",
       "2        Luxury Air Mattress   \n",
       "3             Pillows Insert   \n",
       "4  Milk Frother Handheld\\r\\n   \n",
       "\n",
       "                                              Review  \n",
       "0  I ordered a king size set. My only criticism w...  \n",
       "1  I loved the waterproof sac, although the openi...  \n",
       "2  This mattress had a small hole in the top of i...  \n",
       "3  This is the best throw pillow fillers on Amazo...  \n",
       "4   I loved this product. But they only seem to l...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940ce7c",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92dff22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943237a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace None by your own value and justify\n",
    "llm = ChatOpenAI(temperature=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdcdb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a query that would take a variable to describe any product\n",
    "prompt = ChatPromptTemplate.from_template( \"Write a product summary for {product}. \"  \n",
    "    \"write about highlights and issues. Be short and concise\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7abc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old code\n",
    "# chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Corrected code\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad44d1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The Kindle is a portable e-reader that provides access to thousands of books, magazines, and newspapers. With a high-resolution display and long battery life, it's perfect for avid readers on the go. However, some users may find the lack of a color display limiting for certain types of content.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 24, 'total_tokens': 83, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3fef7bc9-d807-40a1-bec4-58d700c10e02-0', usage_metadata={'input_tokens': 24, 'output_tokens': 59, 'total_tokens': 83})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Kindle\"\n",
    "chain.invoke(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03469",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febee243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f31aa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KK\\AppData\\Local\\Temp\\ipykernel_18996\\458262902.py:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain_one = LLMChain(llm=llm, prompt=first_prompt)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a product summary for {product}. \"  \n",
    "    \"write about highlights and issues. Be short and concise\"\n",
    "    #Repeat of the initial query \n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f5d5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Based on the review {review}, rate the quality of the product from zero to five\"\n",
    "    #Write the second prompt query that takes an input variable whose input will come from the previous prompt\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c1eb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE OF 'SimpleSequentialChain' to combine 2 prompts\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78458efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mThe Kindle is a popular e-reader that allows users to easily download and read books, magazines, and newspapers. With a lightweight design and long battery life, it is perfect for reading on-the-go. The high-resolution display and adjustable font sizes make reading easy on the eyes. However, some users have reported issues with the device freezing or glitches in the software. Overall, the Kindle is a great option for avid readers looking for a convenient way to access a wide variety of reading materials.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mI would rate the Kindle e-reader a 4 out of 5. It offers a lot of great features for readers such as a lightweight design, long battery life, high-resolution display, and adjustable font sizes. However, the reported issues with freezing and glitches in the software bring down the overall quality slightly.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Kindle',\n",
       " 'output': 'I would rate the Kindle e-reader a 4 out of 5. It offers a lot of great features for readers such as a lightweight design, long battery life, high-resolution display, and adjustable font sizes. However, the reported issues with freezing and glitches in the software bring down the overall quality slightly.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.invoke(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd59bda-9d02-44e7-b3d6-2bec61b99d8f",
   "metadata": {},
   "source": [
    "**Repeat the above twice for different products**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ce18c",
   "metadata": {},
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c129ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "016187ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "  \"Translate this review {review} into german\"\n",
    "  #This prompt should translate a review\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key='ger_review' #Give a name to your output\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fb0730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"summarize this review {review} in less than 10 words in german.\"\n",
    "    #Write a promplt to summarize a review\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key='ger_short_review' #give a name to this output\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6accf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english or other language\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate {ger_short_review} into English\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key='en_review'\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a46121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message that take as inputs the two previous prompts' variables\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Rate the quality of the translation from {ger_short_review} into {en_review} from zero to ten.\"\n",
    ")\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key='rate_trans'\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89603117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=['ger_review', 'ger_short_review', 'en_review', 'rate_trans'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51b04f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review': \"This mattress had a small hole in the top of it (took forever to find where it was), and the patches that they provide did not work, maybe because it's the top of the mattress where it's kind of like fabric and a patch won't stick. Maybe I got unlucky with a defective mattress, but where's quality assurance for this company? That flat out should not happen. Emphasis on flat. Cause that's what the mattress was. Seriously horrible experience, ruined my friend's stay with me. Then they make you ship it back instead of just providing a refund, which is also super annoying to pack up an air mattress and take it to the UPS store. This company is the worst, and this mattress is the worst.\",\n",
       " 'ger_review': 'Diese Matratze hatte ein kleines Loch oben drauf (hat ewig gedauert, um herauszufinden, wo es war), und die mitgelieferten Flicken haben nicht funktioniert, vielleicht weil es die Oberseite der Matratze ist, wo es eher wie Stoff ist und ein Flicken nicht haften bleibt. Vielleicht hatte ich Pech mit einer defekten Matratze, aber wo ist die Qualitätskontrolle für dieses Unternehmen? Das sollte einfach nicht passieren. Betonung auf \"flach\". Weil das war die Matratze. Wirklich schreckliche Erfahrung, hat den Aufenthalt meines Freundes bei mir ruiniert. Dann verlangen sie auch noch, dass man sie zurückschickt, anstatt einfach eine Rückerstattung anzubieten, was auch super ärgerlich ist, eine Luftmatratze zu verpacken und zum UPS-Laden zu bringen. Dieses Unternehmen ist das Schlimmste, und diese Matratze ist das Schlimmste.',\n",
       " 'ger_short_review': 'Defekter Luftmatratze, schlechte Qualitätssicherung, schlechte Kundenerfahrung.',\n",
       " 'en_review': 'Defective air mattress, poor quality control, bad customer experience.',\n",
       " 'rate_trans': 'I would rate the translation a 9 out of 10. It accurately conveys the original message and maintains the tone and meaning of the original text.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[2]\n",
    "overall_chain.invoke(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187cf07-458a-4226-bec7-3dec7ee47af2",
   "metadata": {},
   "source": [
    "**Repeat the above twice for different products or reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041ea4c",
   "metadata": {},
   "source": [
    "## Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ade83f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "default_template = \"\"\"You are a helpful assistant. \n",
    "Provide a clear, helpful answer.\n",
    "\n",
    "Here is a Question: \n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f590e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"history\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer_science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DEFAULT\",\n",
    "        \"description\": \"Good for general questions\",\n",
    "        \"prompt_template\": default_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31b06fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3f50bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8eefec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expert chains\n",
    "expert_chains = {}\n",
    "for info in prompt_infos:\n",
    "    prompt = ChatPromptTemplate.from_template(template=info[\"prompt_template\"])\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    expert_chains[info[\"name\"]] = chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b6d6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create router prompt\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "router_template = \"\"\"Given the following question, select the most appropriate category.\n",
    "Available categories:\n",
    "{destinations}\n",
    "\n",
    "Select exactly one category from: physics, math, history, computer_science, or DEFAULT.\n",
    "Respond with only the category name.\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_template(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3eb0b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define router output type\n",
    "class RouterOutput(TypedDict):\n",
    "    destination: Literal[\"physics\", \"math\", \"history\", \"computer_science\", \"DEFAULT\"]\n",
    "\n",
    "# Create router chain\n",
    "router_chain = router_prompt | llm.with_structured_output(RouterOutput) | itemgetter(\"destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "118d0a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is black body radiation?\n",
      "Answer: Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized physical body that absorbs all incident electromagnetic radiation. The radiation emitted by a black body depends only on its temperature and follows a specific distribution known as Planck's law. This radiation is characterized by a continuous spectrum of wavelengths and intensities, with the peak intensity shifting to shorter wavelengths as the temperature of the black body increases. Black body radiation plays a key role in understanding concepts such as thermal radiation and the quantization of energy in quantum mechanics.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What is 2+2\n",
      "Answer: Thank you for the compliment! The answer to the question \"What is 2+2?\" is 4. This can be solved by simply adding the two numbers together.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What caused World War I?\n",
      "Answer: World War I was caused by a combination of factors, including militarism, alliances, imperialism, and nationalism. The assassination of Archduke Franz Ferdinand of Austria-Hungary in 1914 was the immediate trigger for the war, but underlying tensions and rivalries between European powers had been building for years. The complex system of alliances meant that when one country declared war, others were drawn in as well. Additionally, the competition for colonies and resources fueled tensions between European powers. Nationalism also played a role, as countries sought to assert their dominance and protect their interests. Overall, World War I was a result of a combination of long-standing rivalries, alliances, and tensions that ultimately erupted into a global conflict.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Explain Big O notation\n",
      "Answer: Big O notation is a mathematical notation used in computer science to describe the performance or complexity of an algorithm. It represents the upper bound of the worst-case scenario for the time or space complexity of an algorithm.\n",
      "\n",
      "In simpler terms, Big O notation helps us understand how the runtime of an algorithm grows as the input size increases. It allows us to compare different algorithms and determine which one is more efficient in terms of time and space.\n",
      "\n",
      "For example, if an algorithm has a time complexity of O(n), it means that the runtime of the algorithm grows linearly with the size of the input. If it has a time complexity of O(n^2), it means that the runtime grows quadratically with the input size.\n",
      "\n",
      "Overall, Big O notation is a powerful tool for analyzing and comparing algorithms, helping us make informed decisions about which algorithm to use in different scenarios.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What is the forecast for today?\n",
      "Answer: I'm sorry, but I am unable to provide real-time weather forecasts. I recommend checking a reliable weather website or app for the most up-to-date information on today's forecast.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Explain what principle component analysis is using linear algebra\n",
      "Answer: Principal component analysis (PCA) is a technique used in statistics and data analysis to reduce the dimensionality of a dataset while preserving as much variance as possible. In other words, PCA helps to identify the most important features or components of a dataset.\n",
      "\n",
      "In linear algebra terms, PCA involves finding the eigenvectors and eigenvalues of the covariance matrix of the dataset. The eigenvectors represent the directions of maximum variance in the data, while the eigenvalues represent the amount of variance along each eigenvector.\n",
      "\n",
      "To perform PCA using linear algebra, the steps typically involve:\n",
      "1. Standardizing the data by subtracting the mean and dividing by the standard deviation of each feature.\n",
      "2. Computing the covariance matrix of the standardized data.\n",
      "3. Finding the eigenvectors and eigenvalues of the covariance matrix.\n",
      "4. Sorting the eigenvectors by their corresponding eigenvalues in descending order to identify the principal components.\n",
      "5. Projecting the data onto the principal components to reduce the dimensionality of the dataset.\n",
      "\n",
      "By using linear algebra techniques to perform PCA, we can effectively identify the most important features in a dataset and reduce its complexity while retaining as much information as possible.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create final chain\n",
    "def route_to_chain(inputs: dict) -> str:\n",
    "    destination = inputs[\"destination\"]\n",
    "    query = inputs[\"query\"]\n",
    "    return expert_chains[destination].invoke({\"input\": query})\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"query\": RunnablePassthrough(),\n",
    "        \"destination\": lambda x: router_chain.invoke({\"destinations\": destinations_str, \"question\": x})\n",
    "    }\n",
    "    | RunnableLambda(route_to_chain)\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with different types of questions\n",
    "    questions = [\n",
    "        \"What is black body radiation?\", \n",
    "        \"What is 2+2\",      \n",
    "        \"What caused World War I?\",       \n",
    "        \"Explain Big O notation\",     \n",
    "        \"What is the forecast for today?\", \n",
    "        \"Explain what principle component analysis is using linear algebra\",\n",
    "       # \"What are cells?\" #still breaks it \n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        result = chain.invoke(question)\n",
    "        print(f\"Answer: {result}\\n\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b717379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2ca8f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Principal component analysis (PCA) is a technique used in statistics and data analysis to reduce the dimensionality of a dataset while preserving as much variance as possible. In linear algebra terms, PCA involves finding the eigenvectors and eigenvalues of the covariance matrix of the dataset.\\n\\nTo perform PCA using linear algebra, we first center the data by subtracting the mean of each feature from the dataset. Then, we calculate the covariance matrix of the centered data. The covariance matrix represents the relationships between the different features in the dataset.\\n\\nNext, we find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance in the data, while the eigenvalues represent the amount of variance explained by each eigenvector.\\n\\nFinally, we can project the data onto the eigenvectors with the highest eigenvalues to reduce the dimensionality of the dataset. This allows us to represent the data in a lower-dimensional space while retaining as much variance as possible.\\n\\nIn summary, PCA in linear algebra involves finding the eigenvectors and eigenvalues of the covariance matrix of the dataset to reduce the dimensionality of the data while preserving variance.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Explain what principle component analysis is using linear algebra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0c60b-7ae0-453e-9467-142d8dafee6e",
   "metadata": {},
   "source": [
    "**Repeat the above at least once for different inputs and chains executions - Be creative!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
