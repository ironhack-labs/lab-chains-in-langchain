{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# Lab | Chains in LangChain\n",
    "\n",
    "## Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41e2c9a-67c6-4504-a767-6aa90ed32ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:28:49.642021Z",
     "iopub.status.busy": "2024-10-20T22:28:49.641137Z",
     "iopub.status.idle": "2024-10-20T22:29:00.390426Z",
     "shell.execute_reply": "2024-10-20T22:29:00.389295Z",
     "shell.execute_reply.started": "2024-10-20T22:28:49.641994Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7956a301-b007-43aa-86c4-0669718e6dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:29:26.193112Z",
     "iopub.status.busy": "2024-10-20T22:29:26.192186Z",
     "iopub.status.idle": "2024-10-20T22:29:35.956110Z",
     "shell.execute_reply": "2024-10-20T22:29:35.955019Z",
     "shell.execute_reply.started": "2024-10-20T22:29:26.193060Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541eb2f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:35:44.854143Z",
     "iopub.status.busy": "2024-10-20T22:35:44.853807Z",
     "iopub.status.idle": "2024-10-20T22:35:44.861396Z",
     "shell.execute_reply": "2024-10-20T22:35:44.860488Z",
     "shell.execute_reply.started": "2024-10-20T22:35:44.854117Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:37:55.378868Z",
     "iopub.status.busy": "2024-10-20T22:37:55.378458Z",
     "iopub.status.idle": "2024-10-20T22:37:55.392797Z",
     "shell.execute_reply": "2024-10-20T22:37:55.391968Z",
     "shell.execute_reply.started": "2024-10-20T22:37:55.378834Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv('../IHAI-lessons/000_lesson_data/044_llm/.env'))\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:38:56.389537Z",
     "iopub.status.busy": "2024-10-20T22:38:56.388979Z",
     "iopub.status.idle": "2024-10-20T22:38:56.401182Z",
     "shell.execute_reply": "2024-10-20T22:38:56.400337Z",
     "shell.execute_reply.started": "2024-10-20T22:38:56.389512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a09c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:39:00.534817Z",
     "iopub.status.busy": "2024-10-20T22:39:00.534265Z",
     "iopub.status.idle": "2024-10-20T22:39:00.552515Z",
     "shell.execute_reply": "2024-10-20T22:39:00.551476Z",
     "shell.execute_reply.started": "2024-10-20T22:39:00.534790Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940ce7c",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92dff22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:39:05.197342Z",
     "iopub.status.busy": "2024-10-20T22:39:05.197021Z",
     "iopub.status.idle": "2024-10-20T22:39:06.575826Z",
     "shell.execute_reply": "2024-10-20T22:39:06.574921Z",
     "shell.execute_reply.started": "2024-10-20T22:39:05.197315Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943237a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:39:36.387887Z",
     "iopub.status.busy": "2024-10-20T22:39:36.387019Z",
     "iopub.status.idle": "2024-10-20T22:39:36.588545Z",
     "shell.execute_reply": "2024-10-20T22:39:36.587753Z",
     "shell.execute_reply.started": "2024-10-20T22:39:36.387847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace None by your own value\n",
    "\n",
    "llm = ChatOpenAI(temperature = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdcdb42d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:45:07.865966Z",
     "iopub.status.busy": "2024-10-20T22:45:07.865684Z",
     "iopub.status.idle": "2024-10-20T22:45:07.870091Z",
     "shell.execute_reply": "2024-10-20T22:45:07.869041Z",
     "shell.execute_reply.started": "2024-10-20T22:45:07.865943Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Write a funny product description for {product}. Highlight its key features and benefits. Be short and concise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7abc20b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:45:10.586239Z",
     "iopub.status.busy": "2024-10-20T22:45:10.585938Z",
     "iopub.status.idle": "2024-10-20T22:45:10.590697Z",
     "shell.execute_reply": "2024-10-20T22:45:10.589707Z",
     "shell.execute_reply.started": "2024-10-20T22:45:10.586215Z"
    }
   },
   "outputs": [],
   "source": [
    "# old code \n",
    "# chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# correct code\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44d1fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:45:13.559302Z",
     "iopub.status.busy": "2024-10-20T22:45:13.558982Z",
     "iopub.status.idle": "2024-10-20T22:45:14.614766Z",
     "shell.execute_reply": "2024-10-20T22:45:14.614006Z",
     "shell.execute_reply.started": "2024-10-20T22:45:13.559277Z"
    }
   },
   "outputs": [],
   "source": [
    "product = \"Rubber ducks\"\n",
    "\n",
    "chain.invoke(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03469",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febee243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:47:39.897018Z",
     "iopub.status.busy": "2024-10-20T22:47:39.896687Z",
     "iopub.status.idle": "2024-10-20T22:47:39.901697Z",
     "shell.execute_reply": "2024-10-20T22:47:39.900690Z",
     "shell.execute_reply.started": "2024-10-20T22:47:39.896994Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f31aa8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:47:40.523137Z",
     "iopub.status.busy": "2024-10-20T22:47:40.522694Z",
     "iopub.status.idle": "2024-10-20T22:47:40.693164Z",
     "shell.execute_reply": "2024-10-20T22:47:40.692196Z",
     "shell.execute_reply.started": "2024-10-20T22:47:40.523102Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.9)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\"Write a funny product description for {product}. Highlight its key features and benefits. Be short and concise\")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = first_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f5d5b76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:48:19.771576Z",
     "iopub.status.busy": "2024-10-20T22:48:19.771176Z",
     "iopub.status.idle": "2024-10-20T22:48:19.777416Z",
     "shell.execute_reply": "2024-10-20T22:48:19.776207Z",
     "shell.execute_reply.started": "2024-10-20T22:48:19.771542Z"
    }
   },
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\"Based on the review {review}, rate the quality of this product from zero to ten.\")\n",
    "\n",
    "# chain 2\n",
    "chain_two = second_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c1eb2c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:48:22.017720Z",
     "iopub.status.busy": "2024-10-20T22:48:22.017368Z",
     "iopub.status.idle": "2024-10-20T22:48:22.022565Z",
     "shell.execute_reply": "2024-10-20T22:48:22.021474Z",
     "shell.execute_reply.started": "2024-10-20T22:48:22.017696Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_simple_chain = chain_one | chain_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78458efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T22:48:24.745918Z",
     "iopub.status.busy": "2024-10-20T22:48:24.745573Z"
    }
   },
   "outputs": [],
   "source": [
    "print(overall_simple_chain.invoke(product))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd59bda-9d02-44e7-b3d6-2bec61b99d8f",
   "metadata": {},
   "source": [
    "**Repeat the above twice for different products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st product example\n",
    "\n",
    "product_1 = df.iloc[0][0]  # Queen Size Sheet Set\n",
    "\n",
    "print(overall_simple_chain.invoke(product_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1765d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd product example\n",
    "\n",
    "product_2 = df.iloc[1][0]  # Waterproof Phone Pouch\n",
    "\n",
    "print(overall_simple_chain.invoke(product_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ce18c",
   "metadata": {},
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82d816e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3dc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.9)\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\"Translate this {review} into french.\")\n",
    "\n",
    "chain_one = LLMChain(llm = llm, prompt = first_prompt, \n",
    "                     output_key = 'fr_review'  # Output name\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dee834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template('Based on {fr_review}, summarize this review in less than 10 words in french.')\n",
    "\n",
    "chain_two = LLMChain(llm = llm, prompt = second_prompt, \n",
    "                     output_key = 'fr_short_review'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f890c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english or other language\n",
    "third_prompt = ChatPromptTemplate.from_template(\"Translate {fr_short_review} into english.\")\n",
    "\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm = llm, prompt = third_prompt,\n",
    "                       output_key = 'en_short_review'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd1d0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: follow up message that take as inputs the two previous prompts' variables\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "\"Rate the quality of the translation of {fr_short_review} into {en_short_review} from zero to ten.\"\n",
    ")\n",
    "\n",
    "chain_four = LLMChain(llm = llm, prompt = fourth_prompt,\n",
    "                      output_key = 'rating'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1dfe705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output = English_Review, summary, followup_message\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains = [chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables = [\"review\"],\n",
    "    output_variables = ['fr_short_review', 'en_short_review', 'rating'],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afbee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = df.Review[5]\n",
    "\n",
    "result = (overall_chain.invoke(review))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187cf07-458a-4226-bec7-3dec7ee47af2",
   "metadata": {},
   "source": [
    "**Repeat the above twice for different products or reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st review example\n",
    "\n",
    "product_review_1 = df['Review'][2]  # Luxury Air Mattress\n",
    "\n",
    "result = (overall_chain.invoke(product_review_1))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b251a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd review example\n",
    "\n",
    "product_review_2 = df['Review'][3]  # Pillows Insert\n",
    "\n",
    "result = (overall_chain.invoke(product_review_2))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041ea4c",
   "metadata": {},
   "source": [
    "## Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90b8df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \n",
    "You are great at answering math questions. \n",
    "You are so good because you are able to break down \n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together \n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \n",
    "You have an excellent knowledge of and understanding of people, \n",
    "events and contexts from a range of historical periods. \n",
    "You have the ability to think, reflect, debate, discuss and \n",
    "evaluate the past. You have a respect for historical evidence \n",
    "and the ability to make use of it to support your explanations \n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist. \n",
    "You have a passion for creativity, collaboration, forward-thinking, \n",
    "confidence, strong problem-solving capabilities, understanding \n",
    "of theories and algorithms, and excellent communication skills. \n",
    "You are great at answering coding questions. \n",
    "You are so good because you know how to solve a problem by \n",
    "describing the solution in imperative steps that a machine \n",
    "can easily interpret and you know how to choose a solution \n",
    "that has a good balance between time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9bff5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt_templates = {\n",
    "    info[\"name\"]: ChatPromptTemplate.from_template(info[\"prompt_template\"])\n",
    "    for info in prompt_infos\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7835204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c8b6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51ac097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template = prompt_template)\n",
    "    chain = LLMChain(llm = llm, prompt = prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed40952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "\n",
    "default_chain = LLMChain(llm = llm, prompt = default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2e08722",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \n",
    "language model select the model prompt best suited for the input. \n",
    "You will be given the names of the available prompts and a \n",
    "description of what the prompt is best suited for. You may also \n",
    "revise the original input if you think that revising it will \n",
    "ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt names \n",
    "specified below OR it can be \"DEFAULT\" if the input is not well \n",
    "suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input if you \n",
    "don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e03134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations = destinations_str\n",
    ")\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template = router_template,\n",
    "    input_variables = [\"input\"],\n",
    "    output_parser = RouterOutputParser(),\n",
    ")\n",
    "\n",
    "#router_chain = router_prompt | llm | RunnableLambda(lambda x: x.content) \n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(router_chain = router_chain, \n",
    "                         destination_chains = destination_chains, \n",
    "                         default_chain = default_chain, \n",
    "                         verbose = True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ac13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain.invoke(\"Explain what principle component analysis is using linear algebra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "430a9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0c60b-7ae0-453e-9467-142d8dafee6e",
   "metadata": {},
   "source": [
    "**Repeat the above at least once for different inputs and chains executions - Be creative!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "815c3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Example: Art Domain Router Chain\n",
    "\n",
    "# STEP 1: Art Domain Templates\n",
    "\n",
    "painting_template = \"\"\"You are a renowned art historian specializing in painting. \n",
    "You can analyze and explain the techniques, styles, and historical context of paintings. \n",
    "When unsure, you admit your limitations.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "music_template = \"\"\"You are a musicologist with expertise in musical theory, composition, and history. \n",
    "You explain concepts with clarity and connect them to broader themes in music. \n",
    "If unsure, you acknowledge your limitations.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "literature_template = \"\"\"You are a literature expert with deep knowledge of authors, genres, and literary techniques. \n",
    "You provide insightful interpretations and evaluations of texts, recognizing historical and cultural contexts. \n",
    "If unsure, you admit it.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "cinema_template = \"\"\"You are a film critic and historian with expertise in cinema. \n",
    "You analyze movies' artistic, technical, and cultural aspects and provide in-depth explanations. \n",
    "If unsure, you admit it.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9acef98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Prompt Infos and Templates\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"painting\",\n",
    "        \"description\": \"Good for answering questions about painting\",\n",
    "        \"prompt_template\": painting_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"music\",\n",
    "        \"description\": \"Good for answering questions about music\",\n",
    "        \"prompt_template\": music_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"literature\",\n",
    "        \"description\": \"Good for answering questions about literature\",\n",
    "        \"prompt_template\": literature_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cinema\",\n",
    "        \"description\": \"Good for answering questions about cinema\",\n",
    "        \"prompt_template\": cinema_template\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt_templates = {\n",
    "    info[\"name\"]: ChatPromptTemplate.from_template(info[\"prompt_template\"])\n",
    "    for info in prompt_infos\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43072f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Destination Chains and Router Chain\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature = 0)\n",
    "\n",
    "# Create destination chains\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template = prompt_template)\n",
    "    chain = LLMChain(llm = llm, prompt = prompt)\n",
    "    destination_chains[name] = chain  \n",
    "\n",
    "# Define destinations and router prompt\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm = llm, prompt = default_prompt)\n",
    "\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \n",
    "language model, select the model prompt best suited for the input. \n",
    "You will be given the names of the available prompts and a \n",
    "description of what the prompt is best suited for. You may also \n",
    "revise the original input if you think that revising it will \n",
    "ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt names \n",
    "specified below OR it can be \"DEFAULT\" if the input is not well \n",
    "suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input if you \n",
    "don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d39c6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Combine everything into a MultiPromptChain\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "        destinations = destinations_str), \n",
    "        input_variables = [\"input\"], \n",
    "        output_parser = RouterOutputParser())\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain = router_chain, \n",
    "    destination_chains = destination_chains, \n",
    "    default_chain = default_chain, \n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Different Inputs\n",
    "\n",
    "### Painting Query ###\n",
    "chain.invoke(\"What techniques did Van Gogh use in his paintings?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Music Query ###\n",
    "chain.invoke(\"Can you explain the concept of counterpoint in classical music?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Default Query ### (not working - not using default)\n",
    "chain.invoke(\"What is the relationship between art and technology?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
